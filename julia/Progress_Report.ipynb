{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Report\n",
    "### Statistical Machine Learning\n",
    "\n",
    "\n",
    "### Partiall Completed:\n",
    "\n",
    "***Final Project,*** for final project details see content and external hyperlinks below.\n",
    "Which is PCA and random tree regression on Reduced Biological semi-realistic single neuronal models.\n",
    "\n",
    "Idea is simple:\n",
    "Models that are resistant to being classified as models and not data, serve as better imitations/mimics of data. \n",
    "\n",
    "Attempted stochastic gradient descent regression to optimize neural models but found it less useful than multi-objective optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engadging with R course Content\n",
    "\n",
    "\n",
    "### Assignment 3\n",
    "\n",
    "Done most of assignment 3, in R, and some in julia.\n",
    "Done most of the K-Means assignment in R, also re-implemented a lot in Julia.\n",
    "Found a way to import R objects into Julia name space. Found a way to import Julia from Python. \n",
    "[KMeans native Julia](\"used_cars.ipynb\") In this notebook I reproduce rudimentary graphs from the R course content. I wrangle types, and learn how to apply the K-means algorithm using Julias modules, with limited success.\n",
    "\n",
    "[KMeans native R](\"../rscripts/assignment33.r\")\n",
    "[Showing an interoperable environment that sets up Kmeans problems and data for julia](\"idioms_assignment3_9.ipynb\")\n",
    "\n",
    "\n",
    "\n",
    "### Assignment 6\n",
    "Run R code in Julia, installed relevant Julia packages and wrangled data. Reproduced some figures from the R instructional course content [hw6](hw6_pca.ipynb).\n",
    "\n",
    "\n",
    "### Assignment 8\n",
    "\n",
    "[Assignment 8](hw8.jl). I have identified and installed relevant Julia packages (analogous to R) and found a way to conditionally install them. Note conditional installs in R, seem to be a bit more syntactically verbose.\n",
    "\n",
    "### Assignment 9\n",
    "In native R on Linux, I got H20 working via a JVM but I found this to be a laborious and hard to maintain environment, as the module already complains that it is out of date.\n",
    "[Assignment 9](hw92.jl). I have identified and installed relevant Julia Multi Layered Perceptron (MLP) modules (Flux, analogous to R) and found a way to conditionally install them. Note conditional installs in R, seem to be a bit more syntactically verbose. [In the notebook](MLP_flux.ipynb), I identify that I am getting closer to using Julia Flux to solve a simple linear regression type problem on the Boston housing data set. I needed to learn julia native data wrangling techniques first. In another notebook I find out how to conditionally download files, conditionally install modules\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "### Personal Motivation\n",
    "I am involved in a machine Learning project that is very tangental to this course content, non-the-less I am finding ways to tie in what I am learning from this course into my own machine learning strategies. Evolutionary Search Strategies are often part of a multi-strategy frame work. Evolutionary learning algorithms can be very compatible with a plug and play approach to inserting components from other algorithms.\n",
    "\n",
    "\n",
    "\n",
    "### Background Introduction:\n",
    "\n",
    "The Izhikevich model was published with parameter sets believed to pertain to real biological cell types. However since publication much more specific electro physiological recordings have accumulated. Specific knowledge of electrical behavior of real cells has accumulated, and I will refer to this specific data as . When the “Regular Spiking”, neuron category is compared to specific electro-physiological data for pyramidal cells, it is better to optimize against specific data and find a set of model parameters that more accurately reproduce the data. [Introduction Figure](fp_intro.png)\n",
    "\n",
    "\n",
    "In contrast to other projects that seek to use features to seperate and classify two different categories of things that are hard to tell apart, such that humans can benefit from a fast classification of hard to discern differences in high dimensional spaces. In this project the goal is to use resistance to classification as an indicator of an optimization algorithms success, and to use machine seperation of data categories as an error signal, that directs us to precise locations of model failure. Another way of saying this, is, if a good/fair attempt at machine classification is hard, then then a different machine learning algorithm did a good job. If machine classification is very easy, the optimization algorithm did a poor job.\n",
    "\n",
    "### Model Optimization as a data pre-processing stage.\n",
    "Before Machine Learning and analysis techniques could be applied, we needed to find optimized models. These optimized models can be understood as models that are superior mimics of real experimental biologically derived data.\n",
    "\n",
    "* TODO. Remove un optimized models from the feature space (red points), but retain optimized  models green, and experimental ephysiological data from real cells (blue points). Re-optimize models using L1+L2 regularization of the objective function. \n",
    "\n",
    "* TODO use SciKit learn to find out What are the random forest variance explained variables \n",
    "that result from Random Forest classification? \n",
    "\n",
    "* TODO Draw the decision boundary on the classified models.  \n",
    "\n",
    "se K-means clustering on Feature Space, and to use Random Forests variance explained, to identify variables contributing to variance enough that I no longer can seperate between the data categories.\n",
    "\n",
    "In order illustrate that the optimized models are better imitations of real data, four adaptive Exponential models, and four Izhikevich models each were fitted to four different classes of experimental cells see implementation in ipython notebook [Notebook](https://github.com/russelljjarvis/neuronunit/blob/master/neuronunit/examples/seperate_out_data_new_models.ipynb). These eight fitted models where then fed into a Druckman feature extraction algorithm, and added as data points in \n",
    "a dimension reduced plot of the feature space. Many pre-existing neural models, and some Allen Brain Data where also plotted as contextual data in the same feature space. Ordinarily models and data are easily seperable by using PCA on the druckman feature space.\n",
    "\n",
    "## Project Implementation and Technologies\n",
    "* Python, sklearn, dask were all used for Model Optimization pre-processing steps, and for plotting the models in a dimension reduced feature space. \n",
    "* Models versus Data. Models which are resistant to being classified as models are more successful as models, and better representatives of data. See below.\n",
    "\n",
    "\n",
    "![Dimension Reduced Projection Space](final_project.png)\n",
    "\n",
    "The same data plotted with TSNE spatial embedding.\n",
    "![Dimension Reduced Projection Space TSNE spatial embedding](final_project_tsne.png)\n",
    "\n",
    "* TODO use Kmeans clustering on the dimension reduced space. \n",
    "Identify the features that seperate models and data, by iteratively removing dimensions that are Random Forests variance explained, identifies as a variable that contributes more heavily to data variance. Identifying these explanatory variables, is insightful in the context of modelling theory. Variance explained tells us how good models fail, and in which areas models need improving.\n",
    "\n",
    "This Statistical Machine learning Course, has also informed/improved the Data Optimization/Preprocessing stage. As there is an opturnity to introduce L1+L2 regularization to the multi-objective error function used by the Genetic Algorithm to guide learning of the error hyper-volume. \n",
    "\n",
    "* [Introduced regularization](https://github.com/russelljjarvis/neuronunit/blob/master/neuronunit/optimisation/optimization_management.py#L2049). \n",
    "\n",
    "* TODO: Manually introducing Elastic Net (L1+L2) regularization into a multiobjective error function. That I use in the context of a high dimensional multi-objective optimization problem, solved using Genetic Algorithms in Python.\n",
    "\n",
    "Started to use [Stochastic Gradient Descent](https://github.com/russelljjarvis/neuronunit/blob/master/neuronunit/optimisation/optimization_management.py#L2263-L2303) on the same data that the genetic algorithm evaluates. Problematically SG Descent, does not support Multi-objective optimization, so I chose to use Lasso, ridge regression, and elastic search (L1+L2)/2 regularization combined. The way I do this is to run a genetic algorithm over the data, The genetic algorithm is performing its own type of guided sparse sampling of the data.\n",
    "\n",
    "I simply use the GAs recorded samples and run elastic search over the same samples. If elastic net can converge faster than the GA can on the GAs own samples, it provides a grounds to explore the two ideas:\n",
    "\n",
    "* 1. Elastic regularization should be combined into the operation of the GA.\n",
    "\n",
    "* 2. Grid/Exhaustive search with an elastic regularization might also be good.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mandate for using an R-Julia-SKlearn fushion . \n",
    "Here I justify extra time taken to port R code to Julia and python SKLearn.\n",
    "\n",
    "It is well known that R is okay for somethings but slow at iteration. Julia is very fast and expressive. Computer programs that terminate faster, are statistically more likely to be more energy efficient and have a lower byte code foot print. There is a strong mandate for writing more energy optimal code, as this is more compatible with low power computing. Low power computing is and or should be an industry and environmental priority. Low power computing is compatible with ASUs philosophy of being an innovative university, and one which is environmentally sustainable. Faster program execution, also helps me the program writer to debug programs faster, as I wait less long for program outputs. Julia also supports greek letters in code. As a prospective developer in the IT labor force, its important to be able to compete by being differentiated. Being proficient in only R and Python will not help with that. Therefore there are many strong imperatives to use a Julia-Python-R fushion.\n",
    "\n",
    "Built a docker container that supports reproducing this environment [(Julia+R+Python)](https://github.com/russelljjarvis/ScienceAccessibility/blob/dev/with_julia/Dockerfile) co-calling of code. Something to know about this docker file, is that it uses inheritance. I have done a lot of linux system admin related installation tasks in Dockerfiles this Dockerfile inherits from. It's possible to trace source code lineage by looking at the first line ```FROM base_image```. patterns.\n",
    "* Learned how to use Jupyter-lab jupyter notebooks to document work done.\n",
    "\n",
    "There is additional merit in using Docker, as Docker allows other people to more quickly reproduce my work, without avoidably duplicating another persons encountered package management complexity.\n",
    "\n",
    "I was also able to find more graceful ways of conditionally loading data from remote sources if not available locally, and conditionally installing packages if not already installed. Graceful solutions to these problems in R, seem distant and allusive.\n",
    "\n",
    "As a developer that cares about both efficiency of solutions, and reproducibility, there were some parts of R that made it difficult to envisage as a long term standard, or an effective learning aid.\n",
    "\n",
    "### Conditionally download a file in one line:\n",
    "``` julia\n",
    "isfile(\"susedcars.csv\") || download(\"http://www.rob-mcculloch.org/data/usedcars.csv\")\n",
    "```\n",
    "### Conditional module installs\n",
    "\n",
    "* Conditionally import modules or install them if not available then import them:\n",
    "```julia\n",
    "   try\n",
    "       using CSV;using DataFrames;using SciktLearn;using Knet\n",
    "       using ArgParse;using MLDataUtils;using Plots;using Statistics\n",
    "    catch\n",
    "       using Pkg\n",
    "       Pkg.add(\"CSV\");using CSV\n",
    "       Pkg.add(\"DataFrames\");using DataFrames\n",
    "       Pkg.add(\"SciktLearn\");using SciktLearn\n",
    "       Pkg.add(\"Knet\");using Knet\n",
    "       Pkg.add(\"ArgParse\");using ArgParse\n",
    "       Pkg.add(\"MLDataUtils\");using MLDataUtils\n",
    "       Pkg.add(\"Plots\");using Plots\n",
    "       Pkg.add(\"Statistics\");using Statistics\n",
    "    end\n",
    "    ```\n",
    "\n",
    "I could not find a solution to this problem that was fast to execute, easy to read and used less lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
